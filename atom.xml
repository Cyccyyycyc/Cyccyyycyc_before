<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-10-20T07:52:45.817Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>The Transfer-based Black-box Attack Method by 韦星星</title>
    <link href="http://example.com/2022/10/19/The-Transfer-based-Black-box-Attack-Method-by%E9%9F%A6%E6%98%9F%E6%98%9F/"/>
    <id>http://example.com/2022/10/19/The-Transfer-based-Black-box-Attack-Method-by%E9%9F%A6%E6%98%9F%E6%98%9F/</id>
    <published>2022-10-19T15:57:49.000Z</published>
    <updated>2022-10-20T07:52:45.817Z</updated>
    
    <content type="html"><![CDATA[<p>观后感</p><p><a href="http://scl.sribd.cn/seminar/index.html">Source</a> The 4th Lecture</p><h2 id="基于Spatial-Momentum的迁移性增强方法"><a href="#基于Spatial-Momentum的迁移性增强方法" class="headerlink" title="基于Spatial Momentum的迁移性增强方法"></a>基于Spatial Momentum的迁移性增强方法</h2><h3 id="FGSM-Fast-Gradient-Sign-Method"><a href="#FGSM-Fast-Gradient-Sign-Method" class="headerlink" title="FGSM : Fast Gradient Sign Method"></a>FGSM : Fast Gradient Sign Method</h3><p>白盒场景下，用符号获得梯度方向，在原图增加噪声（对抗扰动）来生成对抗样本，是一种单步迭代攻击。</p><h3 id="I-FGSM-Iterative-Fast-Gradient-Sign-Method"><a href="#I-FGSM-Iterative-Fast-Gradient-Sign-Method" class="headerlink" title="I-FGSM : Iterative Fast Gradient Sign Method"></a>I-FGSM : Iterative Fast Gradient Sign Method</h3><p>多步迭代攻击，拟合效果更好了，但是迁移性不太好（可以从overfit角度直观理解）</p><h4 id="时序累加角度：时间域变换"><a href="#时序累加角度：时间域变换" class="headerlink" title="时序累加角度：时间域变换"></a>时序累加角度：时间域变换</h4><h5 id="MI-FGSM"><a href="#MI-FGSM" class="headerlink" title="MI-FGSM"></a>MI-FGSM</h5><p>时序上的梯度累加：当前梯度+过去梯度</p><h5 id="NI-FGSM"><a href="#NI-FGSM" class="headerlink" title="NI-FGSM"></a>NI-FGSM</h5><h4 id="数据增广角度：空间域变换"><a href="#数据增广角度：空间域变换" class="headerlink" title="数据增广角度：空间域变换"></a>数据增广角度：空间域变换</h4><h5 id="DII-FGSM-Diverse-Inputs"><a href="#DII-FGSM-Diverse-Inputs" class="headerlink" title="DII-FGSM : Diverse Inputs"></a>DII-FGSM : Diverse Inputs</h5><p>从数据增强角度，对输入数据有概率p进行随机大小的resize</p><h5 id="TI-FGSM-Translation-invariant-FGSM"><a href="#TI-FGSM-Translation-invariant-FGSM" class="headerlink" title="TI-FGSM : Translation invariant - FGSM"></a>TI-FGSM : Translation invariant - FGSM</h5><p>从数据增强角度，考虑像素点的领域（高斯）来生成对抗样本。</p><h4 id="时空累加角度"><a href="#时空累加角度" class="headerlink" title="时空累加角度"></a>时空累加角度</h4><h5 id="R-DIMI-FGSM"><a href="#R-DIMI-FGSM" class="headerlink" title="R-DIMI-FGSM"></a>R-DIMI-FGSM</h5><p>考虑梯度在空间上的累加，比较范围更广</p><h2 id="图像检测对抗样本的生成"><a href="#图像检测对抗样本的生成" class="headerlink" title="图像检测对抗样本的生成"></a>图像检测对抗样本的生成</h2><h3 id="现有方法问题"><a href="#现有方法问题" class="headerlink" title="现有方法问题"></a>现有方法问题</h3><p>现有攻击方法常常针对Fast-rcnn，攻击模型中的的分类模块（常常表征高级特征），而有些图像检测模型如yolo模型中无分类模块，则导致迁移性差。</p><ol><li>迁移性差，在一个模型训练生成的对抗样本往往无法成功攻击另一个模型。</li><li>时间复杂度高</li></ol><h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><h4 id="改进描述"><a href="#改进描述" class="headerlink" title="改进描述"></a>改进描述</h4><p>一种想法是对抗样本迁移和模型之间的共性相关。要想增强迁移能力，则应该从模型间的共性切入。</p><p>传统方法是攻击Fast-rcnn模型中的分类模块，而有些模型中不存在分类模块，则分类模块显然不是模型的共同子架构。</p><p>因此，提出基于base network的对抗样本生成，取Fast-rcnn模型中一些普遍应用的架构（如VGG、ResNet一些经典架构…），用attention机制来定位特征层的目标机制，增加feature loss，从中间特征层入手破坏物体特征层的特征（原来是直接攻击模型更后面更高级的分类模块），获得更高的迁移性。</p><h4 id="Q1-：底层特征更共性？"><a href="#Q1-：底层特征更共性？" class="headerlink" title="Q1 ：底层特征更共性？"></a>Q1 ：底层特征更共性？</h4><p>这时候可能有人会提出一个问题，既然攻击迁移性往往和模型之间的共同点高度相关，那为什么不选取破坏更加具有共性的底层特征来获得更大的迁移性呢？回答是这样对图片的破坏较大，会导致对抗样本和原图的距离较远，选择在中间特征层攻击是对图片质量（和原图的相似度）和迁移性的权衡结果。</p><p>进一步解释，神经网络往往是一个放大的过程，在底层添加微小噪声，经过网络不断放大，最终和原图便相去甚远。</p><h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><h3 id="Q1：可以从什么角度切入提高迁移性"><a href="#Q1：可以从什么角度切入提高迁移性" class="headerlink" title="Q1：可以从什么角度切入提高迁移性?"></a>Q1：可以从什么角度切入提高迁移性?</h3><ol><li>从梯度出发，找到一个更泛化的梯度计算的方法</li><li>从模型架构共性出发。现在的模型架构趋于模块化，更容易找到共性，提高迁移性。</li><li>（笔者目前个人认为）从模型拟合数据分布的角度出发。不同模型学到的知识不同，有的模型注重纹理有的注重轮廓。</li></ol><h3 id="Q2-：无法被迁移攻击的模型？"><a href="#Q2-：无法被迁移攻击的模型？" class="headerlink" title="Q2 ：无法被迁移攻击的模型？"></a>Q2 ：无法被迁移攻击的模型？</h3><ol><li>从模型架构共性出发：黑盒场景下，如果一个模型无法被迁移攻击，那么说明本地模型和目标模型相似度极低。这不太符合现实应用。不过脉冲神经网络或许是一个角度。</li><li>从模型拟合数据分布角度出发：往往经过对抗训练的模型会具有更高的鲁棒性，相同架构下往往会更难以被对抗攻击，这表明模型拟合出了一个不一样的分布，这个分布含有更多的知识。（有点像“吃一堑长一智”，模型之前见识过了对抗攻击，便学会了如何应对）</li></ol><h3 id="Q3-：语义角度的迁移攻击？"><a href="#Q3-：语义角度的迁移攻击？" class="headerlink" title="Q3 ：语义角度的迁移攻击？"></a>Q3 ：语义角度的迁移攻击？</h3><p>现有方法往往修改图像亮度、锐度、饱和度等属性（对抗属性）来生成对抗样本。可以进行扩展，尝试修改一些视觉友好的语义属性（比如颜色）来生成对抗样本，思考并探索：哪种属性更易于迁移？</p><h3 id="Q4-：如何寻找共性？"><a href="#Q4-：如何寻找共性？" class="headerlink" title="Q4 ：如何寻找共性？"></a>Q4 ：如何寻找共性？</h3><ol><li>从data的特征空间变换角度，可以寻找不同模型之间的一致性</li><li>从降维解构角度，可以实现降低搜索维度，在低维空间中搜索降低时间复杂度，在高维空间中攻击获得好的攻击效果。</li></ol><h3 id="Q5-：迁移性的理论相关？"><a href="#Q5-：迁移性的理论相关？" class="headerlink" title="Q5 ：迁移性的理论相关？"></a>Q5 ：迁移性的理论相关？</h3><p>目前理论尚待完善。</p><p>关于可迁移性的度量，由于迁移并不独立存在，一定是从一个模型到另一个模型，所以目前也无统一度量方法。</p><h3 id="Q6-：可证明的迁移攻击的防御"><a href="#Q6-：可证明的迁移攻击的防御" class="headerlink" title="Q6 ：可证明的迁移攻击的防御"></a>Q6 ：可证明的迁移攻击的防御</h3><p>由于目前迁移攻击的发展空间巨大（效果很差成功率很低），所以没有相关的防御。一般经过对抗训练就能很好地防御了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;观后感&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://scl.sribd.cn/seminar/index.html&quot;&gt;Source&lt;/a&gt; The 4th Lecture&lt;/p&gt;
&lt;h2 id=&quot;基于Spatial-Momentum的迁移性增强方法&quot;&gt;&lt;a href=&quot;#</summary>
      
    
    
    
    <category term="AISP" scheme="http://example.com/categories/AISP/"/>
    
    
    <category term="对抗样本" scheme="http://example.com/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>Deepfake by 吕思伟</title>
    <link href="http://example.com/2022/10/18/Deepfake-by%E5%90%95%E6%80%9D%E4%BC%9F/"/>
    <id>http://example.com/2022/10/18/Deepfake-by%E5%90%95%E6%80%9D%E4%BC%9F/</id>
    <published>2022-10-18T13:55:02.000Z</published>
    <updated>2022-10-20T07:50:07.825Z</updated>
    
    <content type="html"><![CDATA[<p>观后感</p><p><a href="http://scl.sribd.cn/seminar/index.html">Source</a> The 3td Lecture</p><h1 id="DEEP-FAKE"><a href="#DEEP-FAKE" class="headerlink" title="DEEP FAKE"></a>DEEP FAKE</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ol><li>硬件近年来发展迅速，甚至超过了摩尔定律描述的速度。</li><li>互联网用户多，数据体量庞大，且传播较快较广泛。</li><li>AI发展迅速（其实是被大数据和高效的计算资源推着发展的）</li></ol><h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><h3 id="GAN-based-Image"><a href="#GAN-based-Image" class="headerlink" title="GAN-based Image"></a>GAN-based Image</h3><p>生成的fake_images有如下目标：</p><ol><li>质量高，有逼真的特征细节纹理</li><li>种类多</li></ol><h3 id="DNN-based-Speech"><a href="#DNN-based-Speech" class="headerlink" title="DNN-based Speech"></a>DNN-based Speech</h3><ol><li>VC (voice conversion)：可以理解为风格转换，A的内容＋B的风格</li><li>TTS (Text to Speech)</li></ol><h3 id="AE-based-Video"><a href="#AE-based-Video" class="headerlink" title="AE-based Video"></a>AE-based Video</h3><p>理解为逐帧进行替换（风格转换）</p><p>Decoder 分离identity and message，Encode进行一个重新添加。</p><p>问题：耗费较多训练资源，目前还需要比较多的人为调整，而调整操作必然留下痕迹，这也利于检测。</p><h2 id="Impact"><a href="#Impact" class="headerlink" title="Impact"></a>Impact</h2><h3 id="Positive"><a href="#Positive" class="headerlink" title="Positive"></a>Positive</h3><ol><li>电影高难度动作、或者一些场景都可以进行生成</li><li>游戏、远程教育可以通过生成提高交互性</li><li>利于数字匿名化，隐藏个人的identity，可用于个人隐私保护</li><li>生成更具多样性的数据集</li></ol><h3 id="Negative-main"><a href="#Negative-main" class="headerlink" title="Negative(main)"></a>Negative(main)</h3><ol><li>制造假象，混淆视听，误导大众做出错误决策</li><li>降低公众对媒体的信任感，俗称一颗老鼠屎坏了一锅粥</li><li>信息量变大了，含金量并没有提高，信息流变得更为复杂。</li></ol><h2 id="Detection"><a href="#Detection" class="headerlink" title="Detection"></a>Detection</h2><p>虚假信息其实一直都存在，只是随着AI的发展，虚假信息流增强，更难以分辨且威胁性更大，由此Deep-Fake Detection愈发重要。</p><p>下图为吕思伟教授在讲座中给出的DeepFake Detection分类</p><p><img src=".././images/deepfake-video-detection.jpg" alt="deepfake-video-detection"></p><p>首先明确DeepFake检测是一个二分类问题。</p><p>其次可分为Single-modality 和 Multi-modality。</p><p>目前主流是Single-modality  –&gt;  Frame-based  –&gt;  Data-driven的检测手段。</p><p>Cue可以理解为Feature：</p><ol><li>Signal cues：不关心内容，只关心数据本身构成。如Post-processing方法，即为检测生成fake_images后贴到视频里的后处理痕迹，鉴定是否为真。</li><li>Semantic cues：关心数据内容本身是否符合规律。如人脸各个组成的朝向是否正确，如眨眼频率。（个人感觉听起来可延展性较弱，貌似只能针对特定数据特定分布，而且开源后攻击者就可以注意到这个规律，生成符合特定规律的图像就可以骗过检测模型了）</li></ol><p>总体感觉，没有一个普适性的、统一的、可延展的完善的检测方法，较为零碎。</p><h2 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge"></a>Challenge</h2><ol><li>可解释性：随着Deep-Fake的负面影响越来越严重，除了需要检测出”鉴定为假“的结果，在实际应用中，往往不能单纯依靠机器判别，还需要给出假的理由。</li><li>目前检测方法从不同角度切入，种类很多，但可扩展性不强，缺乏一个统一的检测方法。攻击者可以很容易针对专门检测方法进行调整，绕过检测防御。</li><li>很多检测方法误判率较高。实际应用中容易影响用户体验感。</li><li>目前的检测手段实际上是一种事后的被动防御。现在很多人都有一致的想法，利用后门去破坏Deep-Fake的生成，进行主动防御。</li></ol><h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h2><ol><li>生成范围逐渐扩展。不再局限于人脸，逐渐生成四肢、全身…</li><li>共同生成音效＋视频帧，更加逼真</li><li>逐步减少对数据的依赖。比如由单张图片生成动态的视频，由低维构建高维场景。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;观后感&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://scl.sribd.cn/seminar/index.html&quot;&gt;Source&lt;/a&gt; The 3td Lecture&lt;/p&gt;
&lt;h1 id=&quot;DEEP-FAKE&quot;&gt;&lt;a href=&quot;#DEEP-FAKE&quot; class=</summary>
      
    
    
    
    <category term="AISP" scheme="http://example.com/categories/AISP/"/>
    
    
    <category term="DeepFake" scheme="http://example.com/tags/DeepFake/"/>
    
  </entry>
  
  <entry>
    <title>深度学习反脆弱技术的攻防和测评By刘祥龙</title>
    <link href="http://example.com/2022/10/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%8D%E8%84%86%E5%BC%B1%E6%8A%80%E6%9C%AF%E7%9A%84%E6%94%BB%E9%98%B2%E5%92%8C%E6%B5%8B%E8%AF%84By%E5%88%98%E7%A5%A5%E9%BE%99/"/>
    <id>http://example.com/2022/10/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%8D%E8%84%86%E5%BC%B1%E6%8A%80%E6%9C%AF%E7%9A%84%E6%94%BB%E9%98%B2%E5%92%8C%E6%B5%8B%E8%AF%84By%E5%88%98%E7%A5%A5%E9%BE%99/</id>
    <published>2022-10-18T11:55:09.000Z</published>
    <updated>2022-10-20T07:52:53.894Z</updated>
    
    <content type="html"><![CDATA[<p>Source:</p><p><a href="http://scl.sribd.cn/seminar/index.html">论坛网站</a>The 2nd Lecture</p><h1 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h1><h2 id="安全挑战"><a href="#安全挑战" class="headerlink" title="安全挑战"></a>安全挑战</h2><p>非人为刻意引发<br>网络安全，公共安全（安检、自动驾驶），国防安全（侦察、遥感监测）等</p><p>人为刻意构造的全新类型攻击<br>对抗样本、噪音污染、数据投毒、数据伪造、后门攻击…</p><h1 id="对抗样本"><a href="#对抗样本" class="headerlink" title="对抗样本"></a>对抗样本</h1><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p>一类被恶意设计来攻击AI模型的样本</p><ol><li>与真实样本的差异不易感知</li><li>可以导致模型进行错误的判断</li></ol><p>“脆弱性在深度学习中具有普遍性”（Nature 2019）<br>本次主要从对抗样本角度出发关注深度学习的脆弱性。</p><h2 id="数字世界中的对抗样本"><a href="#数字世界中的对抗样本" class="headerlink" title="数字世界中的对抗样本"></a>数字世界中的对抗样本</h2><h3 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h3><ol><li>微小扰动，不易觉察</li><li>语义不变而欺骗模型</li></ol><h3 id="攻击分类"><a href="#攻击分类" class="headerlink" title="攻击分类"></a>攻击分类</h3><ol><li>黑盒攻击</li><li>白盒攻击</li></ol><h4 id="FGSM-attack-2014-基于梯度的攻击"><a href="#FGSM-attack-2014-基于梯度的攻击" class="headerlink" title="FGSM attack:2014,基于梯度的攻击"></a><a href="https://arxiv.org/abs/1412.6572">FGSM attack</a>:2014,基于梯度的攻击</h4><p>攻击假设：白盒，可以获得模型反向传播的梯度符号</p><p>特点<br>1.fast<br>2.sign</p><h4 id="C-amp-W-attack-2017-基于优化的攻击"><a href="#C-amp-W-attack-2017-基于优化的攻击" class="headerlink" title="C&amp;W attack:2017,基于优化的攻击"></a><a href="https://arxiv.org/abs/1608.04644">C&amp;W attack</a>:2017,基于优化的攻击</h4><p>攻击假设：白盒，攻击者需要获得模型数据</p><p>数学理解<br>D：distance<br>C：classification<br>f: 目标函数。当且仅当 f(x+δ)≤0时, C(x+δ)=t</p><p>函数连续：因为要进行优化，所以目标函数需要是光滑连续有梯度的。<br>slow：因为涉及多步优化计算w，所以速度相对较慢。</p><h4 id="PBBA-2017-基于迁移的攻击"><a href="#PBBA-2017-基于迁移的攻击" class="headerlink" title="PBBA:2017,基于迁移的攻击"></a><a href="https://arxiv.org/abs/1602.02697">PBBA</a>:2017,基于迁移的攻击</h4><ol><li>攻击假设：黑盒</li><li>对代理模型的攻击迁移到其它模型</li></ol><h4 id="AdvGAN-2018-基于模型的攻击"><a href="#AdvGAN-2018-基于模型的攻击" class="headerlink" title="AdvGAN:2018,基于模型的攻击"></a><a href="https://arxiv.org/abs/1801.02610">AdvGAN</a>:2018,基于模型的攻击</h4><p>攻击假设：白盒生成，需要获得受害者模型数据来计算adv-L</p><h4 id="其它任务"><a href="#其它任务" class="headerlink" title="其它任务"></a>其它任务</h4><h5 id="Video-Analysis"><a href="#Video-Analysis" class="headerlink" title="Video Analysis"></a>Video Analysis</h5><p>视频逐帧攻击</p><h5 id="Speech-Recognition"><a href="#Speech-Recognition" class="headerlink" title="Speech Recognition"></a>Speech Recognition</h5><h5 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h5><p>自然语言处理领域的对抗样本：<br>对于人类，语序不影响阅读，而文本字母顺序的调换会让模型输出错误的结果。</p><h5 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h5><p>强化学习领域对抗样本的运用实际上是对策略的攻击<br>一种理解是模型本身不够完善，没有学习到如何应对这个策略</p><h3 id="防御"><a href="#防御" class="headerlink" title="防御"></a>防御</h3><h2 id="物理世界中的对抗样本"><a href="#物理世界中的对抗样本" class="headerlink" title="物理世界中的对抗样本"></a>物理世界中的对抗样本</h2><p>物理世界对抗样本：改造物理实体以进行攻击<br>受限于：感知器质量、光照强度、远近距离…<br>与数字世界对抗样本相比，物理世界对抗样本具有黑盒特性，更复杂，危险性更大</p><p>根据数字世界和物理世界的差异，给出对抗样本泛化定义：<br>1.对于人类，视觉上具备友好性 For human, it disguises as a normal example.<br>2.对于模型具有攻击性，可以欺骗模型 For models, it misleads the model predictions</p><h2 id="反脆弱技术体系"><a href="#反脆弱技术体系" class="headerlink" title="反脆弱技术体系"></a>反脆弱技术体系</h2><h3 id="脆弱性原理"><a href="#脆弱性原理" class="headerlink" title="脆弱性原理"></a>脆弱性原理</h3><p>从关键决策路径动态地来看：<br>关键攻击路径刻画了从输入端到决策输出端错误输出的传播路径，这是对模型泛化应用影响最大的路径。<br>表明神经网络中存在脆弱单元，脆弱路径。</p><p>从注意力机制来看：<br>模式识别存在一定偏好，可能对特定的特征（如纹理）有一定偏好（理解为容易激活）</p><h3 id="脆弱性检测"><a href="#脆弱性检测" class="headerlink" title="脆弱性检测"></a>脆弱性检测</h3><p>深度学习网络的对抗鲁棒性和自然噪音鲁棒性往往呈现正相关。提高对抗鲁棒性利于整体鲁棒性的优化，这需要完备数据集的支撑。</p><p>问题：人工智能要想获得广泛使用，成为基础设施，就要有可靠性的保证。<br>挑战：建立完善的评估指标、技术规范和工具集，去测试其模型的可靠性。</p><h3 id="反脆弱加固"><a href="#反脆弱加固" class="headerlink" title="反脆弱加固"></a>反脆弱加固</h3><h4 id="数据端"><a href="#数据端" class="headerlink" title="数据端"></a>数据端</h4><p>过滤有害数据，但没有优化模型本身的反脆弱能力。</p><h5 id="1-污染检测"><a href="#1-污染检测" class="headerlink" title="1.污染检测"></a>1.污染检测</h5><p>对于数据进行domain迁移，数据增强，提高模型泛化能力</p><h5 id="2-污染抑制"><a href="#2-污染抑制" class="headerlink" title="2.污染抑制"></a>2.污染抑制</h5><p>增加防御补丁</p><h5 id="3-污染抑制"><a href="#3-污染抑制" class="headerlink" title="3.污染抑制"></a>3.污染抑制</h5><p>利用W-Distance来进行数据增强，提高模型泛化能力</p><h4 id="模型端：提高鲁棒性"><a href="#模型端：提高鲁棒性" class="headerlink" title="模型端：提高鲁棒性"></a>模型端：提高鲁棒性</h4><h5 id="1-训练加固"><a href="#1-训练加固" class="headerlink" title="1.训练加固"></a>1.训练加固</h5><p>模型单元增强：在中间层注入多样化的对抗噪音，提高鲁棒性，使其学习到更多的语义信息</p><h5 id="2-结构优化："><a href="#2-结构优化：" class="headerlink" title="2.结构优化："></a>2.结构优化：</h5><p>抑制脆弱路径：剪枝，压缩，稀疏化，量化，一定程度上可以抑制噪音<br>中心加权归一化BN进行神经网络模型数据分布整合，提高模型分布的稳定性和收敛性，改善了曲率</p><h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><ul><li>如何平衡模型的精确性和鲁棒性？<br>模型的精确性和鲁棒性使多因素共同作用下的结果，应该全面分析不同因素作用，综合考量设计优化目标</li><li>一种设想：大网络下的特定子网络结构具有鲁棒性</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Source:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://scl.sribd.cn/seminar/index.html&quot;&gt;论坛网站&lt;/a&gt;The 2nd Lecture&lt;/p&gt;
&lt;h1 id=&quot;引入&quot;&gt;&lt;a href=&quot;#引入&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="AISP" scheme="http://example.com/categories/AISP/"/>
    
    
    <category term="对抗样本" scheme="http://example.com/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>数字图像处理</title>
    <link href="http://example.com/2022/10/18/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    <id>http://example.com/2022/10/18/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/</id>
    <published>2022-10-18T11:32:23.000Z</published>
    <updated>2022-10-20T08:06:23.632Z</updated>
    
    <content type="html"><![CDATA[<h3 id="图像基础"><a href="#图像基础" class="headerlink" title="图像基础"></a>图像基础</h3><h4 id="图片表示"><a href="#图片表示" class="headerlink" title="图片表示"></a>图片表示</h4><h5 id="二值图"><a href="#二值图" class="headerlink" title="二值图"></a>二值图</h5><p>只有2种取值</p><h5 id="灰度图"><a href="#灰度图" class="headerlink" title="灰度图"></a>灰度图</h5><p>unit8</p><p>8位灰度图（0~255）</p><p>二维矩阵（一个通道）</p><h5 id="彩色图"><a href="#彩色图" class="headerlink" title="彩色图"></a>彩色图</h5><p>三维矩阵（RGB三个通道）</p><p>真彩色</p><h3 id="通道的分离和合并"><a href="#通道的分离和合并" class="headerlink" title="通道的分离和合并"></a>通道的分离和合并</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img_bgr = cv.imread(img_path)</span><br><span class="line"><span class="comment"># 通道分离</span></span><br><span class="line">b, g, r = cv.split(img_bgr)</span><br><span class="line"><span class="comment"># 通道合并</span></span><br><span class="line">img_rgb = cv.merge([r, g, b])</span><br></pre></td></tr></table></figure><h3 id="彩色图转换成灰度图"><a href="#彩色图转换成灰度图" class="headerlink" title="彩色图转换成灰度图"></a>彩色图转换成灰度图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 三通道按权值加权 0.299 0.587 0.114</span></span><br><span class="line">gray1 = <span class="number">0.299</span> * r + <span class="number">0.587</span> *g + <span class="number">0.114</span> *b</span><br><span class="line"><span class="comment"># dtype = uint8</span></span><br><span class="line">gray2 = np.uint8(gray1)</span><br><span class="line">gray3 = gray1.astype(np.uint8)</span><br><span class="line"></span><br><span class="line">gray4 = cv.cvtColor(img_bgr, cv.COLOR_BGR2GRAY)</span><br></pre></td></tr></table></figure><h3 id="图像二值化"><a href="#图像二值化" class="headerlink" title="图像二值化"></a>图像二值化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">thresh = <span class="number">125</span></span><br><span class="line">gray4[gray4 &gt; thresh] = <span class="number">255</span></span><br><span class="line">gray4[gray4 &lt;= thresh] = <span class="number">0</span></span><br><span class="line"><span class="comment"># gray4 已经被二值化</span></span><br><span class="line"></span><br><span class="line">ignore, img_bin = cv.threshold(gray_uint8_img, th1, th2, cv.THRESH_BINARY)</span><br></pre></td></tr></table></figure><h3 id="图像运算"><a href="#图像运算" class="headerlink" title="图像运算"></a>图像运算</h3><h5 id="图像相加"><a href="#图像相加" class="headerlink" title="图像相加"></a>图像相加</h5><p>混合图像、添加噪声</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dtype = float64</span></span><br><span class="line">img_add1 = cv.add(img1*<span class="number">0.5</span>, img2*<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dtype = uint8</span></span><br><span class="line">img_add2 = cv.addWeighted(img1, alpha, img2, beta, gamma)</span><br></pre></td></tr></table></figure><h5 id="图像相减"><a href="#图像相减" class="headerlink" title="图像相减"></a>图像相减</h5><p>消除背景、差影法（比较差异，运动跟踪）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_sub = cv.subtract(img1, img2)</span><br></pre></td></tr></table></figure><h5 id="图像相乘"><a href="#图像相乘" class="headerlink" title="图像相乘"></a>图像相乘</h5><p>掩膜mask</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = cv.multiply(img1, img2)</span><br></pre></td></tr></table></figure><h5 id="图像相除"><a href="#图像相除" class="headerlink" title="图像相除"></a>图像相除</h5><p>校正设备、比较差异</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = cv.divide(img1, img2)</span><br></pre></td></tr></table></figure><h3 id="图像变换"><a href="#图像变换" class="headerlink" title="图像变换"></a>图像变换</h3><h5 id="线性变换"><a href="#线性变换" class="headerlink" title="线性变换"></a>线性变换</h5><p>$$<br>s=b+kr<br>$$</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = cv.convertScaleAbs(img, alpha=1, beta=0)</span><br></pre></td></tr></table></figure><h5 id="非线性变换"><a href="#非线性变换" class="headerlink" title="非线性变换"></a>非线性变换</h5><p>$$<br>s=a+\frac{ln(r+1)}{blnc}<br>$$</p><h5 id="Gamma变换"><a href="#Gamma变换" class="headerlink" title="Gamma变换"></a>Gamma变换</h5><p>$$<br>s=cr^y<br>$$</p><p>y越大图像越亮</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img = img / <span class="number">255</span></span><br><span class="line">img = np.power(img, y) * <span class="number">255</span></span><br></pre></td></tr></table></figure><h3 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h3><h5 id="裁剪"><a href="#裁剪" class="headerlink" title="裁剪"></a>裁剪</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy</span></span><br><span class="line">img = cv.imread(img_path)</span><br><span class="line"><span class="comment"># h * w * c</span></span><br><span class="line"><span class="comment"># y * x *c</span></span><br><span class="line">img = img[<span class="number">20</span>:<span class="number">100</span>, <span class="number">100</span>:<span class="number">200</span>, :]</span><br></pre></td></tr></table></figure><h5 id="放缩"><a href="#放缩" class="headerlink" title="放缩"></a>放缩</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># OpenCv</span></span><br><span class="line"><span class="comment"># (x, y)=(w, h)=(500,400)</span></span><br><span class="line">img = cv.resize(img, (<span class="number">500</span>, <span class="number">400</span>))</span><br></pre></td></tr></table></figure><h5 id="平移"><a href="#平移" class="headerlink" title="平移"></a>平移</h5><p>仿射变换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 坐标的映射矩阵M</span></span><br><span class="line">M = np.array([...], dtype=np.float32)</span><br><span class="line">cv.warpAffine(img, M, dsize)</span><br></pre></td></tr></table></figure><h5 id="错切变换"><a href="#错切变换" class="headerlink" title="错切变换"></a>错切变换</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">M = np.array([...], dtype=np.float32)</span><br><span class="line">img = cv.warpAffine(img, M, dsize)</span><br></pre></td></tr></table></figure><h5 id="镜像变换"><a href="#镜像变换" class="headerlink" title="镜像变换"></a>镜像变换</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵</span></span><br><span class="line">M = np.array([...], dtype=np.float32)</span><br><span class="line">img = cv.warpAffine(img, M, dsize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 垂直镜像</span></span><br><span class="line">cv.flip(img, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 水平镜像</span></span><br><span class="line">cv.flip(img, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同时进行</span></span><br><span class="line">cv.flit(img, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h5 id="旋转变换"><a href="#旋转变换" class="headerlink" title="旋转变换"></a>旋转变换</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 旋转矩阵</span></span><br><span class="line">M = np.array([...], dtype=np.float32)</span><br><span class="line">img = cv.warpAffine(img, M, dsize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># M = cv.getRotationMatrix2D(center, angle, scale)</span></span><br><span class="line">h, w, c = img.shape</span><br><span class="line"><span class="comment"># center = (x, y)</span></span><br><span class="line">M = cv.getRotationMatrix2D((w//<span class="number">2</span>, h//<span class="number">2</span>), <span class="number">45</span>)</span><br><span class="line">img = cv.warpAffine(img, M, dsize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顺时针逆时针旋转90°</span></span><br><span class="line">img_rotate = cv.rotate(img, cv.ROTATE_90_CLOCKWISE)</span><br></pre></td></tr></table></figure><h5 id="透视变换"><a href="#透视变换" class="headerlink" title="透视变换"></a>透视变换</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">M = cv.getPerspectiveTransform(src, dst)</span><br><span class="line">img = cv.warpPerspective(img, M, dsize)</span><br></pre></td></tr></table></figure><h5 id="小总结"><a href="#小总结" class="headerlink" title="小总结"></a>小总结</h5><p>像素值没变，像素位置变了。<br>所以实际上计算了一个坐标变换的矩阵M。</p><h5 id="最近邻插值"><a href="#最近邻插值" class="headerlink" title="最近邻插值"></a>最近邻插值</h5><p>逆向思维：小图插值变大图 —&gt; 大图变小图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img1 = cv.resize(img, dsize, interpolation=cv.INTER_NEAREST)</span><br></pre></td></tr></table></figure><h5 id="双线性插值"><a href="#双线性插值" class="headerlink" title="双线性插值"></a>双线性插值</h5><p>考虑邻近的像素点，按照权值计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img1 = cv.resize(img, dsize, interpolation=cv.INTER_LINEAR_EXACT)</span><br></pre></td></tr></table></figure><h3 id="图像模糊"><a href="#图像模糊" class="headerlink" title="图像模糊"></a>图像模糊</h3><h5 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = cv.filter2D(img, -<span class="number">1</span>, kernel)</span><br></pre></td></tr></table></figure><h5 id="均值模糊"><a href="#均值模糊" class="headerlink" title="均值模糊"></a>均值模糊</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv.blur(img, (<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">cv.boxFilter(img, -<span class="number">1</span>, (<span class="number">5</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure><h5 id="中值滤波"><a href="#中值滤波" class="headerlink" title="中值滤波"></a>中值滤波</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv.medianBlur(img, <span class="number">3</span>)  <span class="comment"># 奇数</span></span><br></pre></td></tr></table></figure><h5 id="高斯模糊"><a href="#高斯模糊" class="headerlink" title="高斯模糊"></a>高斯模糊</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigma 方差</span></span><br><span class="line"><span class="comment"># 方差小则copy原图</span></span><br><span class="line"><span class="comment"># 方差大则和均值滤波差不多</span></span><br><span class="line">cv.GaussianBlur(img, (<span class="number">5</span>,<span class="number">5</span>), sigmaX)</span><br></pre></td></tr></table></figure><h5 id="双边滤波"><a href="#双边滤波" class="headerlink" title="双边滤波"></a>双边滤波</h5><p>一般模糊会丢失边缘信息，而双边滤波可以保留边缘高频信息，平滑颜色相近的地方。</p><p>需要一直更新卷积核的值：</p><ol><li>距离越远，加权值越小</li><li>颜色差异越大，加权值越小</li></ol><p>缺点：对高频噪声无滤波效果</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv.bilateralFilter(img, -1, sigmaColor=50, sigmaSpace=3)</span><br></pre></td></tr></table></figure><h3 id="图像边缘"><a href="#图像边缘" class="headerlink" title="图像边缘"></a>图像边缘</h3><p>……</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;图像基础&quot;&gt;&lt;a href=&quot;#图像基础&quot; class=&quot;headerlink&quot; title=&quot;图像基础&quot;&gt;&lt;/a&gt;图像基础&lt;/h3&gt;&lt;h4 id=&quot;图片表示&quot;&gt;&lt;a href=&quot;#图片表示&quot; class=&quot;headerlink&quot; title=&quot;图片表示&quot;&gt;&lt;/a</summary>
      
    
    
    
    <category term="cv" scheme="http://example.com/categories/cv/"/>
    
    
  </entry>
  
</feed>
